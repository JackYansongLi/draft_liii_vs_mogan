\section{Limitations of \TeX{} in User Experience Design}
\label{sec:ux}

\TeX{} and its derivative, the \LaTeX{} ecosystem, have long occupied a central position in the field of academic typesetting. However, the overall user experience has failed to evolve in sync with the advancement of computing environments and user expectations. From the continuous bloating of distribution sizes to the performance and interaction barriers imposed by the compilation model, and further to the long-standing engineering defects in language design, the \TeX{} ecosystem falls short in modern writing contexts---particularly regarding "usability," "maintainability," and "adaptability to modern workflows." This section analyzes the limitations of \TeX{}'s user experience design from multiple dimensions, including deployment costs, language structure, and practical user experience.

\subsection{Issues with Distribution Scale and Deployment Models}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/iso_size.pdf}
\caption{TeX Live ISO Size Trends}
\label{fig:texlive-size}
\end{figure}

Figure~\ref{fig:texlive-size} illustrates the trend in file size for the most popular TeX Live ISO from 2008 to 2025. The horizontal axis represents the year, and the vertical axis represents file size in GB. Starting at 2.4~GB in 2008, the line shows an overall upward trajectory, reaching 5.9~GB by 2025. Despite minor fluctuations (such as a drop to 1.9~GB in 2010 and a slight dip to 3.2~GB in 2018), the long-term trend demonstrates a nearly twofold increase in size, reflecting the continuous expansion of software packages.

This volumetric growth is primarily driven by TeX Live's role as a comprehensive \LaTeX{} distribution, which constantly integrates new fonts, documentation, multi-language support, and packages to adapt to user needs and technological advancements. For instance, while early versions focused on core functionality, later iterations incorporated extensive PDF support, graphics libraries, and extensions, resulting in volume bloat.

However, this continuous expansion warrants critical reflection. Taking TeX Live as an example, a significant portion of its distribution volume consists of documentation sets and font resources. For beginners or light users requiring only basic typesetting functions, such overhead constitutes a clear redundant burden, consuming installation time, disk space, and maintenance effort. Although the \TeX{} ecosystem emphasizes modularity at the package level, the distribution strategy (as illustrated in Figure~\ref{fig:texlive-installer}) still prioritizes full installation as the default. Minimalist installation options, while available, are neither highlighted nor widely adopted. Consequently, most users default to the full version, effectively "normalizing" the issues of bloat and resource waste.

While distributions like MikTeX attempt to address this, a more aggressive promotion of lean versions in default configurations by making documentation, examples, and large font sets optional is far more rational in terms of efficiency, resource utilization, and environmental impact, particularly in network- or storage-constrained scenarios.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/texlive-installer.png}
\caption{TeX Live Installer}
\label{fig:texlive-installer}
\end{figure}

Figure~\ref{fig:texlive-installer} illustrates the complex installation interface. In contrast, Mogan STEM uses an on-demand plugin loading mechanism, where features are only loaded when needed. This significantly reduces the installation size and startup time.

Compounded by its underlying compilation model, \LaTeX{} presents a series of severe usability obstacles. Stemming from inherent contradictions in language design, these issues, manifested as massive installation footprints, slow compilation speeds, and steep learning curves, collectively form a `deterrent barrier' for modern users.

\subsection{Real-World Performance Dilemma}

\LaTeX{}'s performance disadvantages warrant discussion, as they render the system incongruous with fast-paced modern workflows. Here, `performance' is not defined by interaction latency or incremental feedback, but rather serves a typesetting model centered on \textbf{batch processing}. The underlying assumption is that the user provides a relatively complete and stable source file, which the system processes through one or more full compilation cycles to generate quality-controlled output. Under this batch-oriented model, optimization prioritizes the correctness and consistency of the final result, rather than immediate responsiveness during the writing process.

However, this throughput-oriented performance goal aligns poorly with modern documentation patterns that prioritize low latency, localized feedback, and continuous interaction. Frequent, minor modifications during writing often trigger a re-parsing and re-typesetting of the entire document, introducing significant wait times that disrupt the continuity between editing and thinking.

Beyond performance unpredictability within a single platform, \LaTeX{} exhibits significant discrepancies in compilation time across different operating systems. For instance, under identical document and package configurations, compilation on Windows is often noticeably slower than on Linux or Unix-like systems. This disparity stems not from hardware differences, but from underlying factors such as file system efficiency, process instantiation overhead, font and I/O management, and the optimization level of the \TeX{} toolchain for specific platforms.

For users, this cross-platform inconsistency undermines system comprehensibility and predictability. The same project may yield vastly different compilation experiences on a personal computer, a lab server, or a cloud environment, making `performance issues' difficult to reproduce, isolate, or optimize. In collaborative scenarios, this variance amplifies into actual collaboration friction, effectively influencing workflow rhythm and tool selection.

Furthermore, \LaTeX{}'s reliance on a multi-stage compilation workflow exacerbates runtime performance costs. Functions such as bibliography management, indexing, and cross-referencing are typically offloaded to multiple external tools, requiring users to run multiple compilation rounds to achieve stable output. This design choice is not driven by performance optimization, but by the language's inherent inability to express complex dependencies within a single compilation pass. Consequently, the overhead of repeated disk I/O, redundant scanning, and process startup is systematically offloaded onto the user.

\subsection{Intrinsic Weakness: Absence of Engineering Standards}

\LaTeXe{} is fundamentally a collection of macros built upon the \TeX{} macro expansion mechanism. Its abstraction capabilities rely primarily on untyped text substitution and grouping scopes, rather than explicit language-level structures. Although the community has actively advanced the \LaTeX3 project in recent years to ameliorate this situation, the project has not been released as a standalone version. Instead, it follows a strategy of progressive evolution, gradually integrating into the existing \LaTeXe{} kernel. Currently, the \texttt{expl3} programming layer provided by \LaTeX3 serves as the foundation for numerous large-scale packages. Consequently, users often utilize its mechanisms indirectly and unconsciously in their daily workflows. This transition primarily results in improved interface consistency, maintainability, and engineering capabilities, accompanied by a degree of performance enhancement.

However, this progressive integration fails to eliminate the long-standing structural contradictions within \LaTeXe{}'s language design. As a system built on a macro expansion language, its approach to modularity, interface expression, and state management stands in fundamental tension with modern engineering practices. Table~\ref{tab:latex-contradictions} systematically outlines these inherent contradictions and their impact on usability and system stability.

\LaTeX{} lacks formal engineering standards for package development. While the \texttt{lppl} license provides some guidance, there are no enforced standards for:

\begin{itemize}
    \item Package documentation quality
    \item API stability and versioning
    \item Error handling and reporting
    \item Testing and validation
\end{itemize}

This has led to a fragmented ecosystem where package quality varies widely, and compatibility issues are common \cite{interview2021}.

\begin{table}[htbp]
\centering
\caption{Core Structural Contradictions in \LaTeXe{} Language Design and their Engineering Consequences}
\label{tab:latex-contradictions}
\begin{tabular}{p{3.5cm}p{5cm}p{4cm}}
\toprule
\textbf{Contradiction} & \textbf{Description} & \textbf{Impact} \\
\midrule
Global Naming vs.\ Modularity & \LaTeXe{} lacks namespaces; all commands and variables share a global symbol table & High risk of package conflicts; loading order directly affects behavior; system fragility \\
Text Substitution vs.\ Structured Interfaces & Macros are essentially untyped token substitutions, lacking parameter signatures and type systems & Static checking is difficult; function interfaces are uncomposable; parameter passing is error-prone \\
Local State via Grouping vs.\ Compiler Awareness & State management relies on \TeX{}'s grouping and rollback mechanism rather than explicit variable scoping & Compilers cannot resolve scope; state leakage and logical errors are difficult to detect \\
Conventional Interfaces vs.\ Automatic Verification & Interface constraints rely on documentation and voluntary compliance, not language-level enforcement & Parameter types/counts cannot be statically verified; errors are only exposed at runtime \\
Compatibility vs.\ Modern Features & To maintain compatibility with \LaTeXe{}, implementation of new features is cumbersome & Requires reliance on advanced packages like \texttt{xparse} or complex hacks; high learning and maintenance costs \\
\bottomrule
\end{tabular}
\end{table}

In summary, \LaTeXe{} represents a macro language with a severe deficit in engineering rigor. Its system stability, maintainability, and user experience depend heavily on informal conventions and user expertise, rather than structural guarantees provided by the language design itself. This deficiency not only increases the complexity of extension and debugging but also establishes an unavoidable historical burden that constrains future systemic evolution.

\subsection{User Experience Barriers from a Practical Perspective}

The structural contradictions in language design discussed above do not remain abstract concepts. Instead, they directly translate into tangible user experience barriers repeatedly encountered during document composition, package combination, and troubleshooting. These issues do not stem from isolated implementation defects or inadequate documentation, but are intrinsic to the macro expansion mechanism and engineering constraints upon which \LaTeXe{} relies.

\textbf{Lack of Usability and Interface Consistency:} Since command interfaces are constrained by convention rather than language-level mechanisms, \LaTeXe{} lacks a unified standard for parameter syntax, optional argument placement, and starred variants across different packages. A typical example is \verb|\newcommand|, which supports only a single, fixed-position optional argument, whereas more complex interface requirements necessitate the use of packages like \texttt{xparse} (via \verb|\NewDocumentCommand|). This deficiency in interface expressiveness forces users to frequently switch mental models between different packages, significantly increasing the cost of learning and usage.

\textbf{Fragility and Implicit Errors Induced by the Global Namespace:} \LaTeXe{} lacks language-level namespaces and encapsulation mechanisms; all commands and variables share a global symbol table. This design makes it difficult to guarantee independence between packages, with loading order often directly affecting document behavior. In the worst-case scenario, definitions of the same command by different packages may result in ``silent overwriting,'' causing subtle but imperceptible changes in document output, thereby weakening system predictability and reliability.

\textbf{High Opacity in Error Diagnosis and Debugging:} Under an execution model where macro expansion and typesetting decisions are deeply intertwined, error messages are often detached from their root causes. Common alerts like ``\verb|Undefined control sequence|'' or ``\verb|Overfull \hbox|'' typically indicate the surface location where the issue was triggered, rather than the origin of the error. Troubleshooting often forces users to rely on empirical methods like step-by-step commenting and regression testing. This process is both inefficient and difficult to systematize, further exacerbating the maintenance burden for complex documents or large projects.

\textbf{Limited Extensibility and Non-Linear Growth in Implementation Complexity:} When users attempt to implement features with even slight complexity (e.g., multiple optional arguments, starred variants, conditional interfaces, or nested logic), they are often compelled to utilize extensive low-level macro hacks to bypass the language's expressive limitations. Such implementations typically rely on implicit state, special naming conventions, and opaque expansion orders, leading to a significant degradation in code readability and maintainability. Consequently, a linear increase in feature complexity is often accompanied by an exponential rise in implementation complexity.

\textbf{High Learning Costs and the Accumulation of ``Tacit Knowledge'':} Collectively, these issues cause the learning path for \LaTeXe{} to depend heavily on tacit knowledge, including grouping scope rules, internal naming conventions (such as \verb|@|-class commands), macro expansion timing, and execution order. This knowledge is not explicitly expressed through language mechanisms but is scattered across package implementations and community lore, making it difficult for new users to establish a stable, transferable cognitive framework.

\subsection{Contributions and Constraints of \LaTeX3}

The \LaTeX3 project emerged as a response to the structural dilemmas described above. It attempts to introduce a programming layer with distinct software engineering characteristics while preserving the underlying \TeX{} macro expansion model. By systematically standardizing and constraining the macro programming practices of \LaTeXe{} through modular naming conventions, an explicit data type system, and controlled scope management, the project aims to bring order to the system. As its core component, the \texttt{expl3} programming layer improves interface consistency and code maintainability, thereby reducing the risks of naming conflicts and implicit state interference during package integration and extension.

However, the improvements offered by \LaTeX3 remain incremental and limited. Its commitment to long-term backward compatibility with \LaTeXe{} dictates that the new and old programming paradigms coexist for a considerable period. Users are required not only to master the new interface model but also to interact with existing packages and historical conventions. This coexistence complicates the ecosystem and renders the migration process slow and uncertain. It serves as a testament to how the historical baggage accumulated over \LaTeX{}'s long evolution continues to constrain its further engineering development.

\subsection{Limitations of Modern Collaboration Platforms: A Case Study of Overleaf}

In response to \LaTeX{}'s structural deficiencies regarding collaboration support and usability, the academic community has advanced a series of improvement measures. Among these, online collaboration platforms, represented by \textbf{Overleaf}, have emerged as the most influential solution in recent years. By providing an integrated cloud-based environment, Overleaf significantly improves the onboarding experience and multi-user collaboration workflows for \LaTeX{}. However, viewed from the perspective of system architecture, Overleaf's design does not address the fundamental issues of \LaTeX{}'s core model. Its role is closer to the encapsulation and optimization of existing workflows rather than a revolution of the underlying paradigm.

\subsubsection{Key Improvements by Overleaf}

Overleaf has effectively mitigated several obstacles inherent in traditional \LaTeX{} usage through the following mechanisms:

\begin{itemize}
    \item \textbf{Simplified Environment Configuration:} Users are no longer required to install a full \TeX{} distribution (such as TeX Live or MiKTeX) locally. Document composition commences via a web browser, thereby eliminating the technical barrier of local deployment.

    \item \textbf{Automated Compilation Workflow:} The system automatically manages multi-pass compilation and external tool invocations (e.g., BibTeX, MakeIndex, Biber). Users need only trigger a single ``Recompile'' action; the platform's backend manages complex dependencies, reducing the technical knowledge required of the user.

    \item \textbf{Enhanced Collaboration Features:} The platform supports multi-user real-time editing, commenting/annotation, and version history tracking. It provides a foundational experience comparable to modern documentation tools (such as Google Docs), significantly outperforming traditional collaboration methods based on email or Git.
\end{itemize}

These platform-level improvements have significantly expanded the applicability of \LaTeX{} in non-professional typesetting scenarios, such as education and scientific research.

\subsubsection{Constraints of Underlying Architecture}

Despite Overleaf's significant progress in user interface and workflow, its foundation still relies entirely on the traditional \LaTeX{} compilation mechanism~\cite{overleaf_docs}, which introduces several insurmountable limitations:

\begin{itemize}
    \item \textbf{Preview Latency Issues:} The so-called ``real-time preview'' is essentially periodic remote compilation and PDF retrieval, not true instant rendering. When document size increases or complex packages (e.g., \texttt{TikZ}, \texttt{algorithm2e}) are included, compilation time rises significantly, leading to a degraded interactive experience.

    \item \textbf{Limited Conflict Resolution in Collaboration:} Since \LaTeX{} source files are unstructured plain text, the platform cannot parse the document structure at a semantic level. When multiple users simultaneously modify the same paragraph, table, or macro definition, the system can only perform line-based text merging. It is unable to identify logical conflicts, ultimately necessitating manual intervention for resolution.

    \item \textbf{Restricted Functional Extensibility:} Overleaf cannot transcend the boundaries of \LaTeX{}'s own expressive capabilities. For instance, it fails to support true WYSIWYG editing modes, nor does it easily integrate dynamic content (such as interactive charts or real-time data binding). Implementing these features requires a structural overhaul of the typesetting engine, rather than mere adjustments to the frontend interface.
\end{itemize}

\subsubsection{Impact on Trajectory of Technological Evolution}

The success of Overleaf serves as a double-edged sword: while it enhances user experience, it simultaneously masks \LaTeX{}'s inherent technical debt and structural defects, thereby diminishing the motivation to explore next-generation typesetting systems. In contrast, systems such as \TeX{}macs or Typst attempt to refactor the document model and syntax design from the foundation up, yet they struggle to disrupt \LaTeX{}'s dominant position due to entrenched user habits.

Consequently, although Overleaf optimizes the usability of the existing ecosystem and resolves certain surface-level pain points, it objectively establishes a form of ``path dependence,'' delaying the demand for more fundamental technological transformation. This platform-driven optimization renders the legacy system more usable but fails to propel it toward a revolutionary direction characterized by greater structure, extensibility, and interactivity.
