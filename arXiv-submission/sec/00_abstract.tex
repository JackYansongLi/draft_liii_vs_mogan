\begin{abstract}
As large language models (LLMs) increasingly assist scientific writing, limitations and significant token cost of the TeX become more and more visible. This paper analyzes TeX's fundamental defects in compilation and user experience design to illustrate its limitations on compilation efficiency, generated semantics, error localization, and tool ecosystem in the era of LLMs. As an alternative, \textbf{Mogan STEM}, a WYSIWYG structured editor is introduced. Mogan outperforms TeX in the above aspects by its efficient data structure, fast rendering, and on-demand plugin loading. Extensive experiments are conducted to verify the benefits on compilation/rendering time, performance in LLM tasks. What's more, we show that due to Mogan's lower information entropy, it is more efficient to use \texttt{.tmu} (document format of Mogan) to fine-tune LLMs than TeX. Therefore, we launch an appeal for larger experiments on LLM training using the \texttt{.tmu} format.
\end{abstract}
