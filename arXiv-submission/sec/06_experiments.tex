\section{Numerical experiments}
\label{sec:experiments}

To verify the beneficial of using Mogan STEM compared to \LaTeX{}, we designed and conducted experiments on fast compiling/rendering, LLM tasks performance, and fine-tuning.

\subsection{Benchmark on compiling/rendering time}

Limited by the design of \LaTeX{}, the compilation process requires significant time for documents that are rich in cross-references, tables of contents, and bibliographies. We chose 6 papers from arXiv that satisfies the richness as benchmark documents (machine configuration is attached in Appendix~\ref{app:ms}). Note that Mogan STEM is a WYSIWYG editor, therefore the comparison is unfair! The time consumption for Mogan STEM is
\[
t_{\text{compiling}} + t_{\text{rendering}} + t_{\text{IO}},
\]
where $t_{\text{compiling}}$ is the compiling time, $t_{\text{rendering}}$ is the rendering time, and $t_{\text{IO}}$ is the \emph{extra} I/O overhead for WYSIWYG editing.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/full-compile.pdf}
\caption{Benchmark on full compilation. Each bar represents the average of three trials}
\label{fig:full-compile}
\end{figure}

Figure~\ref{fig:full-compile} shows the results. Even with the extra I/O process, Mogan STEM outperforms \LaTeX{} in compiling/rendering time for most documents. Note that for document \texttt{arXiv:2502.17655}, Mogan STEM compile and render slower than \LaTeX{}, the exception is due to the size of the documents, it has 120 pages. Therefore, the I/O overhead accounts for a large proportion.

Another limitation of \LaTeX{} is the slow incremental updates. We also conducted an experiment on incremental updates. The update includes adding new sections, adding tables in some paragraphs, modifying the relations between labels and references, and rearrange the positions of contexts slightly. As shown in Figure~\ref{fig:inc-update}, Mogan STEM shows remarkable advantages over \LaTeX{} when doing incremental update on all 6 documents. Note that for document \texttt{arXiv:2502.17655}, Mogan STEM outperforms \LaTeX{}, which seems to be a contradiction to the result of compiling time experiment. The reason for such ``contradiction'' is due to the I/O overhead, in Mogan STEM, the I/O process only activated when the user open the document. Therefore, for incremental updates, $t_{\text{IO}}=0$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/inc-update.pdf}
\caption{Benchmark on incremental update. Each bar represents the average of three trials}
\label{fig:inc-update}
\end{figure}

Figure~\ref{fig:inc-update} shows the results for incremental updates. Mogan STEM shows remarkable advantages over \LaTeX{} when doing incremental update on all 6 documents. The reason for such ``contradiction'' is due to the I/O overhead, in Mogan STEM, the I/O process is triggered by user's operation, so the I/O overhead is amortized by user's interaction.

\subsection{Performance in LLM tasks}

We highly recommend using \texttt{.tmu} to train LLMs instead of \texttt{.tex}. The highly standardized grammar and structured tree-node tags in \texttt{.tmu} files help the models locate the targets faster, complete the contexts properly, and debug the illed structure efficiently. The beneficial are summarized as three dimensions: locating document structure, merging files with distinct doc-styles, and debugging illed document using error messages, which will be discussed in the rest of this subsection.

\subsubsection{Locating document structure}

To evaluate the LLM's grasp of document structure, we designed tests on 4 LLMs. Each test has 20 questions (attached in Appendix~\ref{app:20ques}) about the article's structure from \texttt{arXiv:2502.17655}. For each answer, we take
\[
u_s = \max\left(0, \begin{cases}
5 - \left\lfloor \frac{T}{1 \times 10^4} \right\rfloor & \text{, right answer} \\
0 & \text{, wrong answer}
\end{cases} \right),
\]
where $T$ is the token usage for the input, thinking, output, and MCP tools, $\sum u_s \in [0, 100]$. The reason for using 10k tokens as a scale is that when LLM had high confidence and located the structure accurately, it always consumed less than 10k tokens per question on our material of experiment. Higher token usage means more loops of thinking and more times of MCP tools using, which represents lower efficiency and costs more money.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/reading.pdf}
\caption{Test on locating document structure}
\label{fig:reading}
\end{figure}

Figure~\ref{fig:reading} illustrates the results. Even if it is not a fair comparison as the LLMs have trained inherently by \LaTeX{} corpus before, Mogan took the lead for the most LLMs. The reason is that the file structure in Mogan have higher information density and even references are updated and written (e.g., \verb|<associate|sec:tree-struc-on-mogan|<tuple|5.1|13>>|, where \verb|sec:tree-struc-on-mogan| is the label name, \verb|5.1| is the section number, and \verb|13| is the page number) in \texttt{.tmu} file after incremental update, while in \LaTeX{}, LLM need to count the enviroment in the whole document in order to get the real displayed enviroment number after \LaTeX{} compilation. So the LLM can locate the enviroment quickly in Mogan.

\subsubsection{Merging files with distinct doc-styles}
\label{sec:doc-style}

We define \emph{doc-style} informally as the style of macro naming and command usage in a document. Documents could have distinct macros aliases, redefined macros, and more. For example, the theorem environments could be named \verb|theorem| in one but \verb|thm| in another. The same macro could have different meaning if \verb|\norm| is defined as \verb|\left\lVert #1 \right\rVert| in one but \verb|\mid #1 \mid| in another. The same command could have different usage if \verb|\R| is defined as \verb|\mathbb{R}| in one but \verb|\textcolor{red}{#1}| in another.

To verify the benefit of generating precise structured and standard grammar for LLM writing, we ask the LLMs to complete two assignments.

Assignment 1 is to generate two files: \texttt{theorems.tex} and \texttt{proofs.tex}. \texttt{theorems.tex} is a large collection of mathematical theorems in a doc-style that includes a lot of newly-defined macros, redefined commands, and packages. \texttt{proofs.tex} is the collection of proofs to the theorems above but disordered and has distinct doc-style. \texttt{proofs.tex} even includes conflict packages compared to the \texttt{theorems.tex}. The connection between these two files is the cross-references of each theorem and equation. The LLM should guarantee that \texttt{theorems.tex} and \texttt{proofs.tex} generated can be compiled successfully alone.

Assignment 2 is to merge the two files generated by each LLM. The merged file should be written in the same doc-style as the leading file \texttt{theorems.tex}. The LLM should guarantee that the merged file can be compiled successfully and the proofs are placed properly below their theorems according to cross-references.

We use Mogan STEM to generate \texttt{theorems.tmu} and \texttt{proofs.tmu} directly from their \texttt{.tex} version, the task in Mogan is similar. The context in both Mogan and \LaTeX{} are identical after rendering, which is guaranteed by the \LaTeX{} importing engine built-in Mogan.

The Assignment 1 has 1 task (generate two files). The Assignment 2 has 4 tasks (merge two files generated by 4 LLMs). For each task, we take
\[
u_m = \max\left(0, \begin{cases}
20 - 2 \times E_{\text{ref}} - \left\lfloor \frac{T}{1 \times 10^4} \right\rfloor - E_{\text{sty}} & \text{, success on the first try} \\
10 - 2 \times E_{\text{ref}} - \left\lfloor \frac{T}{1 \times 10^4} \right\rfloor - E_{\text{sty}} & \text{, success on the second try} \\
0 & \text{, fail within two tries}
\end{cases} \right),
\]
where $T$ is the token usage for the input, thinking, output, and MCP tools, $E_{\text{ref}}$ is the number of reference failing (i.e., "??" appears but compilation succeeds), $E_{\text{sty}}$ is the number where the merged file have the doc-style of \texttt{proofs.tex} (we required LLMs to write merged file in the doc-style of \texttt{theorems.tex}, so other macros alias or redefined macros are not accepted), $\sum u_m \in [0, 100]$.

As illustrated in Figure~\ref{fig:writing}, Mogan gains higher score on all LLMs. The reason is that Mogan files have grammatical consistency so that the LLMs do not need to tackle with conflicts and unify usage from two distinct doc-styles. The hallucinations and randomness from LLMs are strictly limited as well.

\subsubsection{Debugging illed document using error messages}

Debugging illed documents is a common usage of LLM co-writing. We constructed several illed documents (originate from \texttt{arXiv:2502.17655}), feed the error messages to LLMs, and ask them to fix it. The test have 20 illed samples as shown in Table~\ref{tab:ill-dist}.

\begin{table}[htbp]
\centering
\caption{Distribution of illness in samples}
\label{tab:ill-dist}
\begin{tabular}{lc}
\toprule
\textbf{Illness} & \textbf{Amount} \\
\midrule
Unclosed bracket & 4 \\
Unclosed environment & 5 \\
Wrong command usage & 4 \\
Undefined cross-reference & 3 \\
Conflict packages & 2 \\
Self-recursive macros & 2 \\
\bottomrule
\end{tabular}
\end{table}

For each illness, we take
\[
u_d = \max\left(0, \begin{cases}
5 - \left\lfloor \frac{T}{1 \times 10^4} \right\rfloor & \text{, right answer} \\
0 & \text{, wrong answer}
\end{cases} \right),
\]
where $T$ is the token usage for the input, thinking, output, and MCP tools, $\sum u_d \in [0, 100]$, as illustrated in Figure~\ref{fig:debugging}.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/debugging.pdf}
\caption{Test on debugging illed document using error messages}
\label{fig:debugging}
\end{figure}

\LaTeX{} group thinks for a long time to solve most of the error samples. Mogan group locates the problems quickly and solve all of the error samples (only two samples consume more than 10k tokens). The reason is that the illness in \texttt{.tmu} files only influences the closest tree-tag ancestor (as discussed in Section~\ref{sec:tree-struc-on-mogan}). So the error message in Mogan STEM is quiet clear for LLMs to understand and correct them easily. In contrast, \LaTeX{}'s error messages usually detach from their root causes in large documents and the logs are very long.

In fact, \texttt{.tmu} file do not have problems like unclosed environment or self-recursive macros if it is written by Mogan STEM. In addition, Mogan STEM provides a WYSIWYG and intuitive user interface. If there is anything wrong in \texttt{.tmu} file, when opened by Mogan STEM, it is always clear to see where they are. And the rest part can be rendered correctly instead of being terminated in compilation like \LaTeX{}.

\subsection{Efficiency in fine-tuning}
\label{sec:eff-in-sft}

Recall that Mogan uses tree structure while \LaTeX{} uses linear macro flow. Benefit from the tree structure, it is easier for models to predict the new token in Mogan than in \LaTeX{}.

We conducted a parallel supervised fine-tuning (SFT) experiment (machine configuration is shown in Appendix~\ref{app:ms}). We generate 1000 random formulas written in \LaTeX{} and convert them to the Mogan S-expression by Mogan STEM. We guarantee that the formulas in both Mogan and \LaTeX{} are identical after rendering. The formulas cover fractions, radicals, subscripts and superscripts, matrices, piecewise, integrals and summations, limits, logical quantifiers, composite functions, and nested parentheses. Next, we cut the formulas into two parts. We give the prefix part to the model and let it complete the rest.

Figure~\ref{fig:fine-tune} shows the experiment of low rank adaptation (LoRA) based on Qwen2.5-7B-Instruct on 1000 formulas in 289 steps, Mogan group's loss converges to around 0.4 while \LaTeX{}'s converges to around 0.7. The reason is that Mogan's S-expression have lower information entropy. \LaTeX{} document has a lot of syntax noise. For example, the code \verb|\frac{a}{b}| and \verb|{a \over b}| in \LaTeX{} are equivalent after rendering, the code \verb|x^{2}| and \verb|x^2| are also equivalent after rendering. So the model has less certainty to predict the next token in \LaTeX{} compare to Mogan.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figure/fine-tune.pdf}
\caption{Experiment of LoRA based on Qwen2.5-7B-Instruct}
\label{fig:fine-tune}
\end{figure}

Note that \LaTeX{} documents have weaker grammatical consistency than Mogan, It is a burden for the model to predict the proper command in line with a macro definitions in preamble, especially in large documents written in several distinct doc-styles (discussed in Section~\ref{sec:doc-style}) when training.

Moreover, discussions of Mogan versus Markdown are attached in Appendix~\ref{app:vs}.
