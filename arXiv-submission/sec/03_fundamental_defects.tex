\section{Fundamental Defects in \TeX{}'s Compilation Design}
\label{sec:fundamental}

The batch processing compilation model adopted by the \TeX{} system was rational within the computing environment of its inception in the 1970s and 80s. However, in the context of contemporary academic writing, which emphasizes interactivity, instant feedback, and multi-platform publishing, this model exposes profound systemic tension. Its core issues lie in its \textit{one-off processing flow}, \textit{weak semantic representation}, and \textit{delayed error feedback mechanisms}. These problems not only directly impair user experience but fundamentally constrain the evolution of the tool ecosystem.

\subsection{Batch Model Limitations: Unidirectionality, Weak Semantics, and Delayed Feedback}

\TeX{}'s compilation architecture is rooted in the batch processing paradigm. While capable of efficiently handling static documents, it struggles to adapt to the dynamic, iterative needs of modern writing.

\subsubsection{Unidirectional and One-off Processing Flow}

The working mechanism of \TeX{} follows a rigid sequence: Linear Input Reading $\rightarrow$ Macro Expansion $\rightarrow$ Typesetting Calculation $\rightarrow$ Output Generation.

This flow possesses strong unidirectionality and irreversibility. The system does not maintain an intermediate representation amenable to incremental re-computation; instead, it treats input as a continuous stream of instructions. This design leads to:

\textbf{Delayed Manifestation of Global State:} The overall typesetting state is only evaluable after the full compilation cycle concludes. \TeX{}'s typesetting decisions (pagination, float positioning, citation numbering, etc.) rely on global information, which is often only determined after the document has been fully read and executed. Consequently, users cannot verify whether the document is "typeset correctly" before compilation is complete. Listing~\ref{lst:xref-before-def} shows a typical example:

\begin{lstlisting}[caption={Cross-reference before definition},label={lst:xref-before-def}]
\documentclass{ctexart}
\begin{document}

As shown in Equation~\ref{eq:test}.

\newpage

\begin{equation}
  E = mc^2
\label{eq:test}
\end{equation}

\end{document}
\end{lstlisting}

When this code is compiled with \texttt{xelatex} for the first time, \texttt{\textbackslash ref\{eq:test\}} displays as \texttt{??}. This discrepancy occurs because the equation's numbering information remains ungenerated; \TeX{} cannot know the correct reference number during the current compilation pass. Only after the first pass concludes and the numbering information is written to the \texttt{.aux} file can \TeX{} read this information during the second compilation, enabling \texttt{\textbackslash ref\{eq:test\}} to display the correct number.

This requirement for multiple passes demonstrates that \TeX{}'s typesetting decisions are global and latent: the typesetting engine cannot accurately infer cross-references or numbering within the document before the first compilation finishes. Multiple passes are strictly required to obtain the final result.

\textbf{Inability to Implement Incremental Updates:} Local modifications cannot trigger efficient, localized re-computation; instead, the entire compilation process requires repetition. Even if only a single local element in the document is modified, \TeX{} must re-execute the entire input stream from the beginning, re-expanding macros, re-paginating, and re-calculating all layouts. Listing~\ref{lst:incremental} illustrates this issue:

\begin{lstlisting}[caption={Incremental update example},label={lst:incremental}]
\documentclass{ctexart}
\usepackage{lipsum}
\begin{document}

\section{Section 1}
Some text, some text, some text.
\lipsum[2-8]
\section{Section 2}
More text, more text, more text.

\end{document}
\end{lstlisting}

Suppose we delete line 7, \texttt{\textbackslash lipsum[2-8]}, which originally generated multiple paragraphs occupying significant vertical space. Keeping the rest of the code unchanged, semantically, this is merely a local modification to the internal content of the first section; it does not touch the structure of the second section or anything following it.

However, during actual compilation with \texttt{xelatex}, this modification triggers a cascading reaction: the vertical space previously occupied by \texttt{\textbackslash lipsum[2-8]} vanishes, drastically reducing the height of the first section. This alters the pagination results for the entire document. The title of the second section, which might have been on the next page, moves up to the previous page; consequently, page numbers, headers, footers, and the positions of any floating bodies must be recalculated. Despite the modification occurring in just a single line at the beginning of the document, \TeX{} is compelled to re-read the input stream from the start, re-expand all macros, and re-execute the complete pagination algorithm. It is unable to perform a local reflow solely for "Section 1" or the "affected pages."

This example clearly demonstrates that within \TeX{}'s execution model, there is no stable, reusable intermediate typesetting state. Any seemingly local change can alter all subsequent typesetting decisions. Therefore, the system has no choice but to perform a full re-compilation, making the efficient incremental updates expected of modern editors impossible.

\textbf{Lack of Visualization Upon Interruption:} Once compilation is interrupted by an error, users cannot obtain reliable partial results for preview. \TeX{} typically aborts the output process immediately, preventing users from seeing "what has been typeset so far." Listing~\ref{lst:missing-arg} demonstrates this scenario:

\begin{lstlisting}[caption={Missing argument error},label={lst:missing-arg}]
\documentclass{ctexart}
\newcommand{\mycmd}[1]{\textbf{#1}}
\begin{document}
This page contains perfectly correct content.

\mycmd % Forgot to provide argument

The content following this theoretically does not
depend on this command.
\end{document}
\end{lstlisting}

When this document is compiled with \texttt{xelatex}, the process fails to complete, generating only \texttt{.aux} and \texttt{.log} files. The error prompt is \texttt{Runaway argument?}. This occurs because \TeX{} detects an incomplete or missing argument during macro processing (in this case, \texttt{\textbackslash mycmd} lacks a necessary argument), causing macro expansion to exceed its expected scope. Consequently, subsequent content cannot be typeset, and the entire PDF output fails. Users are forced to analyze the log to locate the error, unable to verify the pages that were already successfully typeset.

This characteristic stands in sharp contrast to the incremental update and continuous feedback mechanisms prevalent in modern editors, significantly constraining user iteration efficiency.

\subsubsection{Tight Coupling Between Compilation and Semantic Phases}

In the \TeX{} system, syntax parsing, macro expansion, semantic interpretation, and typesetting decisions do not form a clear stratified structure; instead, they are deeply coupled within a single execution path. Macro expansion assumes the role of "semantic modeling" while simultaneously triggering specific typesetting behaviors, thereby conflating abstract structure with layout details. While this design offered high flexibility in the early days, it has exposed serious deficiencies in the context of modern document engineering.

First, macro commands often bear the dual responsibility of structural semantics and layout control. Taking \texttt{\textbackslash section} as an example, this command should logically represent only the structural semantic of "section hierarchy." However, its implementation dictates the font size, spacing, numbering method, and whether a table of contents entry is generated as shown in Listing~\ref{lst:section-dual}:

\begin{lstlisting}[caption={Section command with dual responsibility},label={lst:section-dual}]
\section{Introduction}
\end{lstlisting}

This single line of code not only declares a new structural node but immediately triggers a series of typesetting side effects, including incrementing counters, formatting the title, writing to the TOC (\texttt{.toc}), and generating bookmarks (if \texttt{hyperref} is loaded). Because these actions occur simultaneously during the macro expansion phase, downstream tools cannot distinguish whether a command is a "structure declaration" or a "concrete typesetting instruction," making it difficult to understand the document structure without executing the code.

Second, the macro definition mechanism itself cannot distinguish between abstract semantics and concrete implementation. This results in the same syntactic form potentially representing either high-level semantics or merely a typesetting shortcut. For example: \texttt{\textbackslash def\textbackslash theorem\#1\{\textbackslash textbf\{Theorem.\} \#1\}}. Superficially, \texttt{\textbackslash theorem} appears to be a semantic structure (a theorem). In reality, it is a simple wrapper for bold text, lacking numbering, cross-referencing, or hierarchical information. In contrast: \texttt{\textbackslash newtheorem\{theorem\}\{Theorem\}} introduces true structural semantics, including automatic numbering, scoping, and citability. However, from the perspective of the \TeX{} engine, there is no essential difference between these two definitions during the macro expansion phase---both are simply "executable text substitution rules." This semantic indistinguishability prevents static analysis tools from determining whether a specific macro represents document structure or merely affects visual appearance.

Furthermore, the conditional expansion mechanism directly influences the final structure of the document during the parsing phase, rather than merely acting as a late-stage typesetting choice. Listing~\ref{lst:conditional-structure} shows an example:

\begin{lstlisting}[caption={Conditional structure},label={lst:conditional-structure}]
\ifdefined\includeappendix
  \section{Appendix}
\fi
\end{lstlisting}

Whether the structural node "Appendix" exists depends entirely on the truth value of a condition at the moment of macro expansion. In other words, the document's logical structure is not a stable, enumerable abstract tree, but a result dynamically generated during execution. Any tool attempting to analyze the document structure without running \TeX{} must completely simulate macro expansion and conditional logic, which is virtually unfeasible in practice.

The aforementioned high degree of coupling leads to multifaceted negative impacts. First, static analysis tools find it difficult to intervene. Due to the lack of clear semantic boundaries, tools cannot reliably construct an Abstract Syntax Tree (AST) or structural model, making advanced features like code refactoring, semantic checking, and consistency verification arduous to implement.

Since \LaTeX{} compilation results are strongly dependent on instruction order, \LaTeX{} introduced engineering improvements to allow users or package authors to insert and execute custom code at specific execution timing points. These mechanisms are not based on an explicit semantic event model, but rather attach to \TeX{}'s sequential execution flow, coordinating behavior between packages and document structure by "intercepting execution timing." This enables functions such as lazy initialization, global parameter patching, auxiliary file processing, and layout adjustment. In the \LaTeX{} core system, the commonly used hook commands primarily include the four types listed in Table~\ref{tab:latex-hooks}:

\begin{table}[htbp]
\centering
\caption{Common \LaTeX{} Hook Directives and Descriptions}
\label{tab:latex-hooks}
\begin{tabular}{lp{9.5cm}}
\toprule
\textbf{Directive} & \textbf{Description} \\
\midrule
\texttt{\textbackslash AtBeginDocument} & Executes code at the beginning of the document body (after \texttt{\textbackslash begin\{document\}}). \\
\texttt{\textbackslash AtEndDocument} & Executes code before the document ends (before \texttt{\textbackslash end\{document\}}). \\
\texttt{\textbackslash AtEndOfPackage} & Executes code immediately after the current package finishes loading. \\
\texttt{\textbackslash AtEndOfClass} & Executes code immediately after the current document class finishes loading. \\
\bottomrule
\end{tabular}
\end{table}

These directives provide a relatively non-invasive method of collaboration between packages and user documents, avoiding the need to forcibly control loading order or directly modify user source code to achieve functional extension.

\subsubsection{Lagged Manifestation and Ambiguous Localization of Errors}

\TeX{}'s error detection mechanism is fundamentally an \textbf{"execution-driven, post hoc"} paradigm. This is a direct consequence of its compilation model, where linear macro expansion is inextricably interwoven with immediate typesetting. Since the system does not maintain a structured intermediate state capable of real-time verification during execution, errors manifest passively only when execution becomes impossible, exhibiting a set of systemic characteristics.

First, error types are highly conflated at the diagnostic level. Structural defects at the semantic level (e.g., mismatched environments, abnormal parameter expansion) are often reported indistinguishably from failures at the typesetting level (e.g., box overflows, mode-switching errors). Consequently, error messages fail to reflect the relevant level of abstraction, significantly increasing debugging difficulty.

Second, the reported location of an error is rarely its actual point of origin. The line numbers and context provided by \TeX{} typically correspond to the point where the system finally "crashed" or stalled, rather than the source where the issue was introduced. Furthermore, the point of interruption and the origin of the error may be separated by a significant distance: an early, local error might only be exposed in a completely different form after multiple cycles of macro expansion and typesetting decisions. The result is a globalization of local issues, where a subtle defect in a single environment or command is sufficient to cause an irreversible interruption of the entire compilation process.

It must be emphasized that this is not an oversight in specific implementation details, but a characteristic rooted in the fundamental design of \TeX{}: the system lacks a structured state model independent of the execution process that enables continuous consistency checking. In other words, \TeX{} possesses no "prior knowledge" regarding the document's validity; it reports failure only when execution becomes untenable. Listing~\ref{lst:nested-section} shows a confusing error case:

\begin{lstlisting}[caption={Nested section commands},label={lst:nested-section}]
\documentclass{ctexart}
\begin{document}
\section{aaa \subsection{bbb}}
\end{document}
\end{lstlisting}

From the perspective of document structure, this usage of \texttt{\textbackslash section} is clearly illegal: a subsection is nested within the parameter of a section title. However, \LaTeX{} does not report this as a "structural nesting error." Instead, the system enters an inconsistent state during the macro expansion and typesetting execution, eventually throwing a confusing error message such as "LaTeX Error: Not allowed in LR mode." This message neither identifies the violating structural relationship nor points to the location where the issue actually occurred.

The following examples highlight a typical characteristic of \TeX{}'s error detection mechanism: a significant offset often exists between the true origin of the error and the reported location. Listing~\ref{lst:unclosed-frac} demonstrates this issue:

\begin{lstlisting}[caption={Unclosed brace in frac},label={lst:unclosed-frac}]
\documentclass{ctexart}
\begin{document}
\begin{equation}
a^3 - b^3 = \left(a-b\right) \left(a^2+ab+b^2
\end{equation}
\begin{equation}
x=\frac{-b\pm \sqrt{b^{2}-4ac}}{2a
\end{equation}
\end{document}
\end{lstlisting}

In the first instance, the second argument of \texttt{\textbackslash frac} lacks a closing brace (e.g., \verb|\frac{-b\pm \sqrt{b^{2}-4ac}}{2a|). Semantically, this is a distinct local structural error. Yet, \TeX{} does not immediately report an error upon reading this line; instead, it continues to treat subsequent input as part of the argument, maintaining math mode and attempting to complete macro expansion. It is not until \texttt{\textbackslash end\{equation\}} is reached that the system detects the failure of the grouping and mode states to converge properly, triggering a fatal "Runaway argument?" error. By this point, the reported location is far removed from the actual occurrence, forcing users to manually backtrack through the context to verify closed groups and locate the root cause.

A similar latency phenomenon occurs with mismatched delimiters, such as using \texttt{\textbackslash left(} while omitting the corresponding \texttt{\textbackslash right)}. In this scenario, \TeX{} persists in waiting for a command to close the extensible delimiter. Superficially, the formula continues to be accepted and parsed, but its internal state is already inconsistent. Ultimately, the error is triggered at \texttt{\textbackslash end\{equation\}}, potentially interrupting compilation with a message like "You can't use \texttt{\textbackslash eqno} in math mode." It is crucial to emphasize that such errors do not directly point to the logical cause ("missing \texttt{\textbackslash right}"); rather, they reflect the inability to finalize the typesetting process at the closing stage. This transforms an issue of local structural incompleteness into a failure at the mode or typesetting level.

Both scenarios illustrate a fundamental constraint: \TeX{} does not maintain a structural state model capable of immediate verification. Instead, it relies on the passive exposure of issues when the execution process fails. This is one of the root causes of its "lagged manifestation and ambiguous localization" of errors.

\begin{remark}
Mogan STEM effectively mitigates this issue at the system level. By introducing implicit \texttt{around} tags and explicit structural wrapping, the editor maintains a verifiable structural state during the input phase, preventing error propagation. See Section~\ref{sec:tree-struc-on-mogan} for details.
\end{remark}

\subsection{Compatibility Dilemma Beneath a Unified Syntax}

To address varying requirements (such as modern fonts and Unicode support), the \LaTeX{} ecosystem has evolved multiple parallel compilation engines, such as \texttt{pdfLaTeX}, \texttt{XeLaTeX} and \texttt{LuaLaTeX}. This differentiation, rather than being a strength, is a manifestation of system design fragmentation.

From a user's perspective, all three engines accept \texttt{.tex} input and adhere to the \LaTeX{} macro interface. However, at the system level, they exhibit fundamental differences in the key aspects summarized in Table~\ref{tab:latex-engines}:

\begin{table}[htbp]
\centering
\caption{Comparison of \LaTeX{} Engines}
\label{tab:latex-engines}
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
\textbf{Dimension} & \textbf{pdfLaTeX} & \textbf{XeLaTeX} & \textbf{LuaLaTeX} \\
\midrule
Native Encoding Support & 8-bit (requires \texttt{inputenc}) & Native Unicode & Native Unicode \\
Font System & Type1 / limited OpenType & System Fonts (OpenType) & OpenType (via Lua) \\
Extension Mechanism & \TeX{} primitives & \TeX{} + Xe\TeX{} primitives & \TeX{} + Lua VM \\
External Programmability & Very Weak & Very Weak & Strong (Lua) \\
\bottomrule
\end{tabular}%
}
\end{table}

These discrepancies imply that the same \texttt{.tex} source file does not correspond to the same semantic execution environment across different engines.

\textbf{Compatibility Burden:} A single document may require switching between engines to resolve specific issues, forcing users to understand the subtle differences and limitations of each engine.

Take the use of Emojis as an example. If a document requires the direct inclusion of Unicode Emojis (e.g., ðŸ™‚ðŸ“Š), \texttt{pdfLaTeX} is mechanically incapable of supporting such characters.\footnote{Some packages such as \texttt{hwemoji} attempt to work around this limitation by pre-compiling emojis into PDF resources and replacing them during compilation. However, this is not native support---i.e., direct rendering from text and font files---but rather a workaround. We still consider \texttt{pdfLaTeX} to be fundamentally incapable of supporting such characters.} While \texttt{XeLaTeX} possesses native Unicode support, in practice, it often degrades Emojis to monochrome glyphs or suffers from missing characters due to insufficient font coverage. \texttt{LuaLaTeX} offers the theoretically most complete support path, handling complex characters via OpenType fonts and the Lua layer. However, this 'enhanced capability' comes at a price: \texttt{LuaLaTeX}'s compilation speed is typically significantly slower than \texttt{XeLaTeX}, and it introduces an additional dependency on the Lua runtime, expanding both the source of errors and the scope of debugging. Under these constraints, a compromise workflow that actually exists in reality involves: compiling the main body with \texttt{XeLaTeX} for speed and layout stability; separately compiling pages or chapters containing Emojis using \texttt{LuaLaTeX}; and finally stitching the output results from different engines together at the PDF level.

A similar compatibility burden appears in the bibliography processing chain. While \LaTeX{} superficially provides a unified \texttt{.bib} data source format, during the actual compilation process, users must choose between different backends like \texttt{bibtex} and \texttt{biber}. These two differ fundamentally in character encoding, sorting rules, and data models as shown in Listing~\ref{lst:bibtex-biber}:

\begin{lstlisting}[caption={BibTeX vs BibLaTeX},label={lst:bibtex-biber}]
\bibliography{ref/refs}   % References compiled using BibTeX
% \printbibliography       % References compiled using BibLaTeX/Biber
\end{lstlisting}

Visually, these commands differ by only a single line, yet they correspond to two nearly incompatible toolchains. The former triggers the traditional \texttt{bibtex} workflow: the \texttt{.bib} file is parsed by \texttt{bibtex} to generate a \texttt{.bbl} file, employing a data model constrained by 8-bit encoding and a rigid \texttt{.bst} styling mechanism. The latter implicitly requires the \texttt{biblatex} package and the \texttt{biber} backend. It adopts an internal Unicode data model while offloading significantly more logic, specifically sorting, filtering, and formatting, to the macro layer.

Consequently, although both methods "appear to use the same \texttt{.bib} file," they diverge significantly in character encoding support, linguistic processing capabilities, style customization, and compilation steps. A smooth migration cannot be achieved by simply swapping commands. From the user's perspective, this implies that bibliography management is not a stable, interchangeable module, but rather deeply coupled with the engine and packages. Users must not only understand \texttt{.bib} syntax but also explicitly identify which call path their document is entering for bibliography processing. This further exacerbates the cognitive burden regarding tool selection and workflow configuration.

\textbf{Ecosystem Fragmentation:} Certain packages support only specific engines, degrading document portability. A common example is the \texttt{fontspec} package, used for loading system fonts (OpenType/TrueType) and Unicode settings. Under pdfLaTeX, this triggers a fatal error (\texttt{cannot-use-pdftex}) because pdf\TeX{} lacks native Unicode support.

\textbf{Increased Cognitive Load:} These factors compel users to learn not just \LaTeX{} syntax, but also how to select and configure the appropriate compilation engine to achieve their desired document output.

\subsection{Alienation of the Tool Ecosystem}

To compensate for the aforementioned core design deficiencies, the \LaTeX{} ecosystem has spawned a series of complex compensatory mechanisms, which have in turn introduced new problems.

\subsubsection{Multi-pass Compilation: A Fragile Stopgap}

In \LaTeX{}, the accurate generation of cross-references, tables of contents, and bibliographies relies on multiple compilation passes to make up for the absence of internal state. The typical workflow involves a first pass that generates auxiliary files (like \texttt{.aux} and \texttt{.toc}) to record state, followed by subsequent passes that read these files to populate cross-references, page numbers, or chapter titles. If the document includes citations, external tools such as BibTeX or Biber must also be integrated into the process. For example, the standard procedure for a document with references often follows the sequence: \texttt{pdflatex} $\rightarrow$ \texttt{bibtex} $\rightarrow$ \texttt{pdflatex} $\rightarrow$ \texttt{pdflatex}.

This mechanism imposes several burdens:

\begin{itemize}
    \item \textbf{State Fragmentation:} Document state is scattered across multiple external files.
    \item \textbf{Cognitive Load:} Users must master complex ``compilation rituals.'' For instance, failure to run BibTeX results in all citation numbers appearing as \texttt{??}.
    \item \textbf{Debugging Difficulties:} Error tracing is notoriously difficult. An illegal character in a \texttt{.bib} file might trigger an error in the \texttt{\textbackslash bibliography\{refs\}} command, causing the error message to be completely detached from the actual root cause.
\end{itemize}

In extreme cases, cross-references remain generated during the first pass, leaving the user with a mass of undefined numbers or empty tables until the multi-pass cycle is complete. This reliance on external side effects to patch internal architectural flaws is, in essence, a form of technical debt.

\subsubsection{Long-Term Suppression of the Tool Ecosystem}

The batch processing design of \TeX{} has stifled the development of peripheral tools. Real-time preview in \LaTeX{} editors is merely a simulation achieved by frequently triggering background compilations, which suffers from significant latency and layout inconsistencies. For example, in documents containing numerous floating objects and formulas, even modifying a small paragraph can trigger a global re-calculation of pagination, causing the preview to lag. Furthermore, features like intelligent code refactoring or syntax completion are virtually impossible to implement effectively without a structured document tree. Most automation tools rely on regular expression matching rather than syntax tree analysis, leading to limited error-checking capabilities and a high propensity for false positives or missed errors.

This structural dilemma is not accidental; rather, it is rooted in the batch-processing paradigm of \LaTeX{} itself. Frank Mittelbach, the technical lead for \LaTeX{}, candidly admitted in an interview~\cite{interview2021}:

\begin{quotation}
PN: "I try to look at this issue from the point of view of global and local, and interactivity is just like a... This is probably a change that happens very fast, and that you worried only about the local stuff, but the separation between local and global in \LaTeX{} seems to be hard."

FMi: "First of all, it is right now hard in \TeX{}\ldots"
\end{quotation}

\subsection{Comparison: Design Paradigms of Structured and Incremental Systems}

Systems like GNU \TeX{}macs adopt a distinct architecture: the document maintains a structured tree representation in memory, where local modifications trigger immediate local repainting, and structural validity checks occur during the editing phase. For instance, modifying a formula or a floating object redraws only the affected region. Cross-references and numbering update instantly, ensuring the user always interacts with a predictable and previewable document state. Errors are isolated within local modules, preventing global system crashes.

This contrast shows that \LaTeX{}'s need for multiple compilation rounds and external tools is not unavoidable and instead results from a compensatory design strategy. These limitations could be completely bypassed by refactoring the architecture. Ultimately, the fundamental problem of \LaTeX{} lies not the quality of its typesetting, but the defects of global dependency and state management inherent in its execution model.
