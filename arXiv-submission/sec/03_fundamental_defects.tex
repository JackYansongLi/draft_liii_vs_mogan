\section{Fundamental Defects in \TeX{}'s Compilation Design}
\label{sec:fundamental}

The batch processing compilation model adopted by the TeX system was rational within the computing environment of its inception in the 1970s and 80s. However, in the context of contemporary academic writing, which emphasizes interactivity, instant feedback, and multi-platform publishing, this model exposes profound systemic tension. Its core issues lie in its \textit{one-off processing flow}, \textit{weak semantic representation}, and \textit{delayed error feedback mechanisms}. These problems not only directly impair user experience but fundamentally constrain the evolution of the tool ecosystem.

\subsection{Batch Model Limitations: Unidirectionality, Weak Semantics, and Delayed Feedback}

TeX's compilation architecture is rooted in the batch processing paradigm. While capable of efficiently handling static documents, it struggles to adapt to the dynamic, iterative needs of modern writing.

\subsubsection{Unidirectional and One-off Processing Flow}

The core workflow of TeX is inherently unidirectional and batch-oriented. It sequentially reads the source file from beginning to end, expanding macros and processing commands in a single pass. This design, while simple and efficient for its original purpose, creates significant limitations for modern document editing workflows.

The unidirectional nature means that once the compiler has processed a section of the document, it cannot go back to modify it based on information that appears later. This is particularly problematic for cross-references, where the target may appear after the reference itself. The typical workaround is to use multiple compilation passes, but this introduces significant overhead and complexity.

\textbf{Delayed Manifestation of Global State:} The overall typesetting state is only evaluable after the full compilation cycle concludes. TeX's typesetting decisions (pagination, float positioning, citation numbering, etc.) rely on global information, which is often only determined after the document has been fully read and executed. Consequently, users cannot verify whether the document is "typeset correctly" before compilation is complete.

\begin{lstlisting}[caption={Cross-reference before definition},label={lst:xref-before-def}]
\documentclass{ctexart}
\begin{document}

As shown in Equation~\ref{eq:test}.

\newpage

\begin{equation}
  E = mc^2
\label{eq:test}
\end{equation}

\end{document}
\end{lstlisting}

When this code is compiled with \texttt{xelatex} for the first time, \texttt{\textbackslash ref\{eq:test\}} displays as \texttt{??}. This discrepancy occurs because the equation's numbering information remains ungenerated; TeX cannot know the correct reference number during the current compilation pass. Only after the first pass concludes and the numbering information is written to the \texttt{.aux} file can TeX read this information during the second compilation, enabling \texttt{\textbackslash ref\{eq:test\}} to display the correct number.

This requirement for multiple passes demonstrates that TeX's typesetting decisions are global and latent: the typesetting engine cannot accurately infer cross-references or numbering within the document before the first compilation finishes. Multiple passes are strictly required to obtain the final result.

\textbf{Inability to Implement Incremental Updates:} Local modifications cannot trigger efficient, localized re-computation; instead, the entire compilation process requires repetition. Even if only a single local element in the document is modified, TeX must re-execute the entire input stream from the beginning, re-expanding macros, re-paginating, and re-calculating all layouts.

\begin{lstlisting}[caption={Incremental update example},label={lst:incremental}]
\documentclass{ctexart}
\usepackage{lipsum}
\begin{document}

\section{Section 1}
Some text, some text, some text.
\lipsum[2-8]
\section{Section 2}
More text, more text, more text.

\end{document}
\end{lstlisting}

Suppose we delete line 7, \texttt{\textbackslash lipsum[2-8]}, which originally generated multiple paragraphs occupying significant vertical space. Keeping the rest of the code unchanged, semantically, this is merely a local modification to the internal content of the first section; it does not touch the structure of the second section or anything following it.

However, during actual compilation with \texttt{xelatex}, this modification triggers a cascading reaction: the vertical space previously occupied by \texttt{\textbackslash lipsum[2-8]} vanishes, drastically reducing the height of the first section. This alters the pagination results for the entire document. The title of the second section, which might have been on the next page, moves up to the previous page; consequently, page numbers, headers, footers, and the positions of any floating bodies must be recalculated. Despite the modification occurring in just a single line at the beginning of the document, TeX is compelled to re-read the input stream from the start, re-expand all macros, and re-execute the complete pagination algorithm. It is unable to perform a local reflow solely for "Section 1" or the "affected pages."

This example clearly demonstrates that within TeX's execution model, there is no stable, reusable intermediate typesetting state. Any seemingly local change can alter all subsequent typesetting decisions. Therefore, the system has no choice but to perform a full re-compilation, making the efficient incremental updates expected of modern editors impossible.

\textbf{Lack of Visualization Upon Interruption:} Once compilation is interrupted by an error, users cannot obtain reliable partial results for preview. TeX typically aborts the output process immediately, preventing users from seeing "what has been typeset so far."

\begin{lstlisting}[caption={Missing argument error},label={lst:missing-arg}]
\documentclass{ctexart}
\newcommand{\mycmd}[1]{\textbf{#1}}
\begin{document}
This page contains perfectly correct content.

\mycmd % Forgot to provide argument

The content following this theoretically does not
depend on this command.
\end{document}
\end{lstlisting}

When this document is compiled with \texttt{xelatex}, the process fails to complete, generating only \texttt{.aux} and \texttt{.log} files. The error prompt is \texttt{Runaway argument?}. This occurs because TeX detects an incomplete or missing argument during macro processing (in this case, \texttt{\textbackslash mycmd} lacks a necessary argument), causing macro expansion to exceed its expected scope. Consequently, subsequent content cannot be typeset, and the entire PDF output fails. Users are forced to analyze the log to locate the error, unable to verify the pages that were already successfully typeset.

This characteristic stands in sharp contrast to the incremental update and continuous feedback mechanisms prevalent in modern editors, significantly constraining user iteration efficiency.

\subsubsection{Tight Coupling Between Compilation and Semantic Phases}

In TeX, the lexical analysis, parsing, and semantic analysis phases are tightly coupled. The macro expansion mechanism, which is central to TeX's operation, operates at the character level without clear separation between these phases. This design choice has several consequences:

\begin{itemize}
    \item \textbf{Fragile Parsing:} The parser must handle expanded macros, making it difficult to provide meaningful error messages.
    \item \textbf{Limited Optimization:} Without clear phase separation, optimizations that could be applied at specific stages are difficult to implement.
    \item \textbf{Debugging Difficulties:} Errors may manifest far from their actual source due to macro expansion.
\end{itemize}

\subsubsection{Lagged Manifestation and Ambiguous Localization of Errors}

TeX's error reporting is notorious for being cryptic and unhelpful. When an error occurs, the compiler often reports it at a location far removed from the actual source of the problem. This is due to several factors:

\begin{enumerate}
    \item Macro expansion can propagate errors across large portions of the document.
    \item The batch processing model means that errors accumulate before being reported.
    \item Error messages are often technical and do not provide actionable guidance.
\end{enumerate}

For example, a missing brace several pages earlier might cause an error message that appears to indicate a problem in a completely different location. This makes debugging LaTeX documents a time-consuming and frustrating experience, especially for users who are not deeply familiar with the system.

\begin{lstlisting}[caption={Missing argument error},label={lst:mycmd-error}]
\documentclass{ctexart}
\newcommand{\mycmd}[1]{\textbf{#1}}
\begin{document}
This page contains perfectly correct content.

\mycmd % Forgot to provide argument

The content following this theoretically does not
depend on this command.
\end{document}
\end{lstlisting}

Listing~\ref{lst:mycmd-error} shows a missing argument error. The error message \texttt{Runaway argument?} may not clearly indicate the actual problem.

\begin{lstlisting}[caption={Nested section commands},label={lst:nested-section}]
\documentclass{ctexart}
\begin{document}
\section{aaa \subsection{bbb}}
\end{document}
\end{lstlisting}

Listing~\ref{lst:nested-section} shows an invalid nested section command, which produces the error: ``LaTeX Error: Not allowed in LR mode.''

\begin{lstlisting}[caption={Unclosed brace in frac},label={lst:unclosed-frac}]
\documentclass{ctexart}
\begin{document}
\begin{equation}
a^3 - b^3 = \left(a-b\right) \left(a^2+ab+b^2
\end{equation}
\begin{equation}
x=\frac{-b\pm \sqrt{b^{2}-4ac}}{2a
\end{equation}
\end{document}
\end{lstlisting}

Listing~\ref{lst:unclosed-frac} shows an unclosed brace in a \verb|\frac| command, which also produces a \texttt{Runaway argument?} error.

Both scenarios illustrate a fundamental constraint: TeX does not maintain a structural state model capable of immediate verification. Instead, it relies on the passive exposure of issues when the execution process fails. This is one of the root causes of its "lagged manifestation and ambiguous localization" of errors.

\begin{remark}
Mogan STEM effectively mitigates this issue at the system level. By introducing implicit \texttt{around} tags and explicit structural wrapping, the editor maintains a verifiable structural state during the input phase, preventing error propagation.
\end{remark}

\subsection{Compatibility Dilemma Beneath a Unified Syntax}

To address varying requirements (such as modern fonts and Unicode support), the LaTeX ecosystem has evolved multiple parallel compilation engines, such as \texttt{pdfLaTeX}, \texttt{XeLaTeX} and \texttt{LuaLaTeX}. This differentiation, rather than being a strength, is a manifestation of system design fragmentation.

From a user's perspective, all three engines accept \texttt{.tex} input and adhere to the LaTeX macro interface. However, at the system level, they exhibit fundamental differences in the following key aspects:

\begin{table}[htbp]
\centering
\caption{Comparison of \LaTeX{} Engines}
\label{tab:latex-engines}
\resizebox{0.99\textwidth}{!}{%
\begin{tabular}{llll}
\toprule
\textbf{Dimension} & \textbf{pdfLaTeX} & \textbf{XeLaTeX} & \textbf{LuaLaTeX} \\
\midrule
Native Encoding Support & 8-bit (requires \texttt{inputenc}) & Native Unicode & Native Unicode \\
Font System & Type1 / limited OpenType & System Fonts (OpenType) & OpenType (via Lua) \\
Extension Mechanism & TeX primitives & TeX + XeTeX primitives & TeX + Lua VM \\
External Programmability & Very Weak & Very Weak & Strong (Lua) \\
\bottomrule
\end{tabular}%
}
\end{table}

\textbf{Compatibility Burden:} A single document may require switching between engines to resolve specific issues, forcing users to understand the subtle differences and limitations of each engine.

Take the use of Emojis as an example. If a document requires the direct inclusion of Unicode Emojis, \texttt{pdfLaTeX} is mechanically incapable of supporting such characters. While \texttt{XeLaTeX} possesses native Unicode support, in practice, it often degrades Emojis to monochrome glyphs or suffers from missing characters due to insufficient font coverage. \texttt{LuaLaTeX} offers the theoretically most complete support path, handling complex characters via OpenType fonts and the Lua layer. However, this 'enhanced capability' comes at a price: \texttt{LuaLaTeX}'s compilation speed is typically significantly slower than \texttt{XeLaTeX}, and it introduces an additional dependency on the Lua runtime, expanding both the source of errors and the scope of debugging. Under these constraints, a compromise workflow that actually exists in reality involves: compiling the main body with \texttt{XeLaTeX} for speed and layout stability; separately compiling pages or chapters containing Emojis using \texttt{LuaLaTeX}; and finally stitching the output results from different engines together at the PDF level.

A similar compatibility burden appears in the bibliography processing chain. While LaTeX superficially provides a unified \texttt{.bib} data source format, during the actual compilation process, users must choose between different backends like \texttt{bibtex} and \texttt{biber}. These two differ fundamentally in character encoding, sorting rules, and data models.

\begin{lstlisting}[caption={BibTeX vs BibLaTeX},label={lst:bibtex-biber}]
\bibliography{ref/refs}   % References compiled using BibTeX
% \printbibliography       % References compiled using BibLaTeX/Biber
\end{lstlisting}

Visually, these commands differ by only a single line, yet they correspond to two nearly incompatible toolchains. The former triggers the traditional \texttt{bibtex} workflow: the \texttt{.bib} file is parsed by \texttt{bibtex} to generate a \texttt{.bbl} file, employing a data model constrained by 8-bit encoding and a rigid \texttt{.bst} styling mechanism. The latter implicitly requires the \texttt{biblatex} package and the \texttt{biber} backend. It adopts an internal Unicode data model while offloading significantly more logic, specifically sorting, filtering, and formatting, to the macro layer.

Consequently, although both methods "appear to use the same \texttt{.bib} file," they diverge significantly in character encoding support, linguistic processing capabilities, style customization, and compilation steps. A smooth migration cannot be achieved by simply swapping commands. From the user's perspective, this implies that bibliography management is not a stable, interchangeable module, but rather deeply coupled with the engine and packages. Users must not only understand \texttt{.bib} syntax but also explicitly identify which call path their document is entering for bibliography processing. This further exacerbates the cognitive burden regarding tool selection and workflow configuration.

\textbf{Ecosystem Fragmentation:} Certain packages support only specific engines, degrading document portability. A common example is the \texttt{fontspec} package, used for loading system fonts (OpenType/TrueType) and Unicode settings. Under pdfLaTeX, this triggers a fatal error (\texttt{cannot-use-pdftex}) because pdfTeX lacks native Unicode support.

\textbf{Increased Cognitive Load:} These factors compel users to learn not just LaTeX syntax, but also how to select and configure the appropriate compilation engine to achieve their desired document output.

\begin{table}[htbp]
\centering
\caption{Common \LaTeX{} Hook Directives and Descriptions}
\label{tab:latex-hooks}
\begin{tabular}{lp{9.5cm}}
\toprule
\textbf{Directive} & \textbf{Description} \\
\midrule
\texttt{\textbackslash AtBeginDocument} & Executes code at the beginning of the document body (after \texttt{\textbackslash begin\{document\}}). \\
\texttt{\textbackslash AtEndDocument} & Executes code before the document ends (before \texttt{\textbackslash end\{document\}}). \\
\texttt{\textbackslash AtEndOfPackage} & Executes code immediately after the current package finishes loading. \\
\texttt{\textbackslash AtEndOfClass} & Executes code immediately after the current document class finishes loading. \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:latex-hooks} shows some common hook directives. While hooks provide a mechanism for packages to coordinate their initialization, they cannot fully resolve the fundamental tension between extensibility and compatibility. The order of package loading becomes critical, and users must often engage in trial-and-error to find a working configuration.

\subsection{Alienation of the Tool Ecosystem}

\subsubsection{Multi-pass Compilation: A Fragile Stopgap}

In LaTeX, the accurate generation of cross-references, tables of contents, and bibliographies relies on multiple compilation passes to make up for the absence of internal state. The typical workflow involves a first pass that generates auxiliary files (like \texttt{.aux} and \texttt{.toc}) to record state, followed by subsequent passes that read these files to populate cross-references, page numbers, or chapter titles.

If the document includes citations, external tools such as BibTeX or Biber must also be integrated into the process. For example, the standard procedure for a document with references often follows the sequence: \texttt{pdflatex} $\rightarrow$ \texttt{bibtex} $\rightarrow$ \texttt{pdflatex} $\rightarrow$ \texttt{pdflatex}.

This mechanism imposes several burdens:

\begin{itemize}
    \item \textbf{State Fragmentation:} Document state is scattered across multiple external files.
    \item \textbf{Cognitive Load:} Users must master complex ``compilation rituals.'' For instance, failure to run BibTeX results in all citation numbers appearing as \texttt{??}.
    \item \textbf{Debugging Difficulties:} Error tracing is notoriously difficult because errors in auxiliary files (e.g., corrupted \texttt{.aux} files) are hard to diagnose.
\end{itemize}

\subsubsection{Long-Term Suppression of the Tool Ecosystem}

The batch processing design of TeX has stifled the development of peripheral tools. Real-time preview in LaTeX editors is merely a simulation achieved by frequently triggering background compilations, which suffers from significant latency and layout inconsistencies. For example, in documents containing numerous floating objects and formulas, even modifying a small paragraph can trigger a global re-calculation of pagination, causing the preview to lag. Furthermore, features like intelligent code refactoring or syntax completion are virtually impossible to implement effectively without a structured document tree. Most automation tools rely on regular expression matching rather than syntax tree analysis, leading to limited error-checking capabilities and a high propensity for false positives or missed errors.

This structural dilemma is not accidental; rather, it is rooted in the batch-processing paradigm of LaTeX itself. Frank Mittelbach, the technical lead for LaTeX, candidly admitted in an interview~\cite{interview2021}:

\begin{quote}
PN: "I try to look at this issue from the point of view of global and local, and interactivity is just like a... This is probably a change that happens very fast, and that you worried only about the local stuff, but the separation between local and global in LaTeX seems to be hard."

FMi: "First of all, it is right now hard in TeX\ldots"
\end{quote}

\subsection{Comparison: Design Paradigms of Structured and Incremental Systems}

Systems like GNU TeXmacs adopt a distinct architecture: the document maintains a structured tree representation in memory, where local modifications trigger immediate local repainting, and structural validity checks occur during the editing phase. For instance, modifying a formula or a floating object redraws only the affected region. Cross-references and numbering update instantly, ensuring the user always interacts with a predictable and previewable document state. Errors are isolated within local modules, preventing global system crashes.

This contrast shows that LaTeX's need for multiple compilation rounds and external tools is not unavoidable and instead results from a compensatory design strategy. These limitations could be completely bypassed by refactoring the architecture. Ultimately, the fundamental problem of LaTeX lies not the quality of its typesetting, but the defects of global dependency and state management inherent in its execution model.
